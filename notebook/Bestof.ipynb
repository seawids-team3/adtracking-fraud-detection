{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What am I doing here?\n",
    "\n",
    "- GBT model : Everything hardcoded to the features and hyperparams \n",
    "chosen during grid search.\n",
    "- Doing three runs on slightly different subsets of training data.\n",
    "- Taking the median of the three models as the answer to upload.\n",
    "\n",
    "TODO:\n",
    "- Is there a way to do probability calibration in PySpark? \n",
    "- Add any good features from Leila's model and see if I can \n",
    "improve my score a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "pyspark 2.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is optional stuff - either pip install watermark\n",
    "# or just comment it out (it just keeps track of what library\n",
    "# versions I have)\n",
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '8g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [ 'os', 'channel', 'app' ]\n",
    "bigrams = [[ 'device', 'os'], \n",
    "           ['device', 'channel'], \n",
    "           ['device', 'app'], \n",
    "           ['channel', 'app']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1 \n",
    "\n",
    "Read the csv file, drop the attributed_time (because I didn't use it in the MVP),\n",
    "and downsample the 0 class to 25% because I'm still on my laptop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = spark.read.csv('../data/train.csv', \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "    df = df.drop('attributed_time')\n",
    "    df = df.sampleBy('is_attributed', fractions={0:.25,1:1.})\n",
    "    \n",
    "    test = spark.read.csv('../data/test.csv', \n",
    "                         header= True, inferSchema=True)\n",
    "\n",
    "    df.write.parquet('../data/checkpoint1.parquet', mode='overwrite')\n",
    "    test.write.parquet('../data/test_checkpoint1.parquet', mode='overwrite')\n",
    "else:\n",
    "    df = spark.read.parquet('../data/checkpoint1.parquet')\n",
    "    test = spark.read.parquet('../data/test_checkpoint1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('click_id', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46575441"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily IP prevalence \n",
    "Because IP addresses get reassigned, need to do these as feature engineering on train and test\n",
    "sets separately.\n",
    "(See the link Elyse posted on the slack.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('doy', \n",
    "                   F.dayofyear('click_time'))\n",
    "test = test.withColumn('doy',\n",
    "                   F.dayofyear('click_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ip_counts = spark.read.parquet('../data/train_ip.parquet')\n",
    "test_ip_counts = spark.read.parquet('../data/test_ip.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(\n",
    "    df_ip_counts[['doy','ip','ip_pct']],\n",
    "    on=['doy','ip'],\n",
    "    how='left'\n",
    ")\n",
    "test = test.join(\n",
    "    test_ip_counts[['doy','ip','ip_pct']],\n",
    "    on=['doy','ip'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doy', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('click_id', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('ip_pct', 'double')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class balancing \n",
    "\n",
    "Downsample the majority class and upsample the minority class. \n",
    "\n",
    "Todo: Should this downsample just be random or by day or by...? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_a = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=111)\n",
    "class1_b = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=222)\n",
    "class1_c = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df.sampleBy('is_attributed', {0:.11}, seed=111).unionAll(class1_a)\n",
    "df_b = df.sampleBy('is_attributed', {0:.11}, seed=222).unionAll(class1_b)\n",
    "df_c = df.sampleBy('is_attributed', {0:.11}, seed=333).unionAll(class1_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting \n",
    "\n",
    "Built count tables except for IP with the full training set rather than the \n",
    "subset. These tables were created in Spark_count_table.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_table( group ):\n",
    "    if type(group) == str:\n",
    "        column_name = group + '_pct' # for example: ip_pct\n",
    "    else:\n",
    "        column_name = \"_\".join(group)  # for example: device_os\n",
    "        \n",
    "    table_name = 'table_' + column_name\n",
    "    counts_sdf = spark.read.parquet(f'../data/{table_name}.parquet')\n",
    "    return counts_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_table( sdf, count_table, group ):\n",
    "    sdf = sdf.join(count_table, group, how='left')\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the count columns with the training data \n",
    "# write everything out to disk so we don't have to redo \n",
    "# feature engineering when all I want to do is tune hyperparameters\n",
    "if True:\n",
    "    for c in unigrams:\n",
    "        ct   = get_count_table( c )\n",
    "        df_a   = join_table(df_a, ct, [c])\n",
    "        df_b   = join_table(df_b, ct, [c])\n",
    "        df_c   = join_table(df_c, ct, [c])\n",
    "        test = join_table(test, ct, [c])\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        ct = get_count_table( bigram )\n",
    "        df_a = join_table(df_a, ct, bigram)\n",
    "        df_b = join_table(df_b, ct, bigram)\n",
    "        df_c = join_table(df_c, ct, bigram)\n",
    "        test = join_table(test, ct, bigram)\n",
    "\n",
    "    df_a.write.parquet('../data/dfa.parquet', mode='overwrite')\n",
    "    df_b.write.parquet('../data/dfb.parquet', mode='overwrite')\n",
    "    df_c.write.parquet('../data/dfc.parquet', mode='overwrite')\n",
    "    test.write.parquet('../data/test_stack.parquet', mode='overwrite')\n",
    "else:\n",
    "    df_a = spark.read.parquet('../data/dfa.parquet')\n",
    "    df_b = spark.read.parquet('../data/dfb.parquet')\n",
    "    df_c = spark.read.parquet('../data/dfc.parquet')\n",
    "    test = spark.read.parquet('../data/test_stack.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_a = df_a.fillna(0)\n",
    "df_b = df_b.fillna(0)\n",
    "df_c = df_c.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1827229|\n",
      "|            0|5069713|\n",
      "+-------------+-------+\n",
      "\n",
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1828581|\n",
      "|            0|5075130|\n",
      "+-------------+-------+\n",
      "\n",
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1825068|\n",
      "|            0|5070512|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sdf in [ df_a, df_b, df_c ]:\n",
    "    sdf.groupby('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the hour column back in because the score went down\n",
    "\n",
    "def add_hour(sdf):\n",
    "    return sdf.withColumn('hour',\n",
    "                F.hour('click_time').astype(T.FloatType()) + \n",
    "                F.minute('click_time').astype(T.FloatType())/60.)\n",
    "    \n",
    "df_a = add_hour(df_a)\n",
    "df_b = add_hour(df_b)\n",
    "df_c = add_hour(df_c)\n",
    "test = add_hour(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model data in format expected by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [ c + '_pct' for c in unigrams ]\n",
    "input_cols += [ 'device_app', 'channel_app']\n",
    "input_cols += ['ip_pct', 'hour' ]\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=input_cols, outputCol = 'features')\n",
    "    \n",
    "if True:\n",
    "    model_a = vec_assembler.transform(df_a).select('is_attributed', 'features')\n",
    "    model_b = vec_assembler.transform(df_b).select('is_attributed', 'features')\n",
    "    model_c = vec_assembler.transform(df_c).select('is_attributed', 'features')\n",
    "    \n",
    "    model_a.write.parquet('../data/model_a.parquet', mode='overwrite')\n",
    "    model_b.write.parquet('../data/model_b.parquet', mode='overwrite')\n",
    "    model_c.write.parquet('../data/model_c.parquet', mode='overwrite')\n",
    "else:\n",
    "    model_a = spark.read.parquet('../data/model_a.parquet')\n",
    "    model_b = spark.read.parquet('../data/model_b.parquet')\n",
    "    model_c = spark.read.parquet('../data/model_c.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'is_attributed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtc = GBTClassifier(\n",
    "    labelCol = 'is_attributed',\n",
    "    maxDepth = 8,\n",
    "    subsamplingRate = 0.8,\n",
    "    featureSubsetStrategy = '5',\n",
    "    maxBins = 64,\n",
    "    stepSize = .16,\n",
    "    maxIter = 10,\n",
    "    minInstancesPerNode = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_a = gbtc.fit(model_a)\n",
    "results_b = gbtc.fit(model_b)\n",
    "results_c = gbtc.fit(model_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'is_attributed', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's bring the test set in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = vec_assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = results_a.transform(test_model)\n",
    "test_b = results_b.transform(test_model)\n",
    "test_c = results_c.transform(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sdf):\n",
    "    sdf = sdf.select('click_id', \n",
    "                       F.col('prediction').astype(T.ShortType()), \n",
    "                       'probability')\n",
    "    sdf.groupby('prediction').count().show()\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|  507111|\n",
      "|         0|18283358|\n",
      "+----------+--------+\n",
      "\n",
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|  503658|\n",
      "|         0|18286811|\n",
      "+----------+--------+\n",
      "\n",
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|  526701|\n",
      "|         0|18263768|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_a = get_prediction(test_a)\n",
    "count_b = get_prediction(test_b)\n",
    "count_c = get_prediction(test_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = T.StructType([\n",
    "    T.StructField('click_id', T.IntegerType()),\n",
    "    T.StructField('prediction', T.ShortType()),\n",
    "    T.StructField('pclass1', T.FloatType())\n",
    "])\n",
    "\n",
    "def save_stuff(x):\n",
    "    return T.Row(click_id=x.click_id, \n",
    "                prediction=x.prediction, \n",
    "                pclass1=float(x.probability[1]))\n",
    "\n",
    "vec_a = count_a.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)\n",
    "vec_b = count_b.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)\n",
    "vec_c = count_c.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the median of the three models as my final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_a = vec_a.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_a') )\n",
    "vec_b = vec_b.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_b') )\n",
    "vec_c = vec_c.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_c') )\n",
    "\n",
    "joined = vec_a.join(vec_b, ['click_id']).join(vec_c, ['click_id'])\n",
    "\n",
    "mySchema = T.StructType([\n",
    "    T.StructField('click_id', T.IntegerType()),\n",
    "    T.StructField('is_attributed', T.FloatType())\n",
    "])\n",
    "\n",
    "from statistics import median\n",
    "def get_predict(x):\n",
    "    return T.Row(click_id=x.click_id,\n",
    "                is_attributed=median([x.vec_a, x.vec_b, x.vec_c]))\n",
    "\n",
    "joined = joined.rdd.map(lambda x: get_predict(x)).toDF(schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.write.csv('../data/vote_results.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
