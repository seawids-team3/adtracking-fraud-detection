{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What am I doing here?\n",
    "\n",
    "- GBT model : Everything hardcoded to the features and hyperparams \n",
    "chosen during grid search.\n",
    "- Doing three runs on slightly different subsets of training data.\n",
    "- Taking the median of the three models as the answer to upload.\n",
    "\n",
    "TODO:\n",
    "- Is there a way to do probability calibration in PySpark? \n",
    "- Add any good features from Leila's model and see if I can \n",
    "improve my score a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark 2.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is optional stuff - either pip install watermark\n",
    "# or just comment it out (it just keeps track of what library\n",
    "# versions I have)\n",
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '8g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [ 'os', 'channel', 'app' ]\n",
    "bigrams = [['device', 'app'], \n",
    "           ['channel', 'app']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1 \n",
    "\n",
    "Read the csv file, drop the attributed_time (because I didn't use it in the MVP),\n",
    "and downsample the 0 class to 25% because I'm still on my laptop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = spark.read.csv('../data/train.csv', \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "    df = df.drop('attributed_time')\n",
    "    df = df.sampleBy('is_attributed', fractions={0:.25,1:1.})\n",
    "    \n",
    "    test = spark.read.csv('../data/test.csv', \n",
    "                         header= True, inferSchema=True)\n",
    "\n",
    "    df.write.parquet('../data/checkpoint1.parquet', mode='overwrite')\n",
    "    test.write.parquet('../data/test_checkpoint1.parquet', mode='overwrite')\n",
    "else:\n",
    "    df = spark.read.parquet('../data/checkpoint1.parquet')\n",
    "    test = spark.read.parquet('../data/test_checkpoint1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('click_id', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46575441"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily IP prevalence \n",
    "Because IP addresses get reassigned, need to do these as feature engineering on train and test\n",
    "sets separately.\n",
    "(See the link Elyse posted on the slack.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('doy', \n",
    "                   F.dayofyear('click_time'))\n",
    "test = test.withColumn('doy',\n",
    "                   F.dayofyear('click_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ip_counts = spark.read.parquet('../data/train_ip.parquet')\n",
    "test_ip_counts = spark.read.parquet('../data/test_ip.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(\n",
    "    df_ip_counts[['doy','ip','ip_pct']],\n",
    "    on=['doy','ip'],\n",
    "    how='left'\n",
    ")\n",
    "test = test.join(\n",
    "    test_ip_counts[['doy','ip','ip_pct']],\n",
    "    on=['doy','ip'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doy', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('click_id', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('ip_pct', 'double')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class balancing \n",
    "\n",
    "Downsample the majority class and upsample the minority class. \n",
    "\n",
    "Todo: Should this downsample just be random or by day or by...? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_a = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=111)\n",
    "class1_b = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=222)\n",
    "class1_c = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df.sampleBy('is_attributed', {0:.11}, seed=111).unionAll(class1_a)\n",
    "df_b = df.sampleBy('is_attributed', {0:.11}, seed=222).unionAll(class1_b)\n",
    "df_c = df.sampleBy('is_attributed', {0:.11}, seed=333).unionAll(class1_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting \n",
    "\n",
    "Built count tables except for IP with the full training set rather than the \n",
    "subset. These tables were created in Spark_count_table.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_table( group ):\n",
    "    if type(group) == str:\n",
    "        column_name = group + '_pct' # for example: ip_pct\n",
    "    else:\n",
    "        column_name = \"_\".join(group)  # for example: device_os\n",
    "        \n",
    "    table_name = 'table_' + column_name\n",
    "    counts_sdf = spark.read.parquet(f'../data/{table_name}.parquet')\n",
    "    return counts_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_table( sdf, count_table, group ):\n",
    "    sdf = sdf.join(count_table, group, how='left')\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add hour column for Leila's new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour(sdf):\n",
    "    return sdf.withColumn('hour',\n",
    "                F.hour('click_time').astype(T.FloatType()) + \n",
    "                F.minute('click_time').astype(T.FloatType())/60.)\n",
    "    \n",
    "df_a = add_hour(df_a)\n",
    "df_b = add_hour(df_b)\n",
    "df_c = add_hour(df_c)\n",
    "test = add_hour(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the count columns with the training data \n",
    "# write everything out to disk so we don't have to redo \n",
    "# feature engineering when all I want to do is tune hyperparameters\n",
    "if True:\n",
    "    for c in unigrams:\n",
    "        ct   = get_count_table( c )\n",
    "        df_a   = join_table(df_a, ct, [c])\n",
    "        df_b   = join_table(df_b, ct, [c])\n",
    "        df_c   = join_table(df_c, ct, [c])\n",
    "        test = join_table(test, ct, [c])\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        ct = get_count_table( bigram )\n",
    "        df_a = join_table(df_a, ct, bigram)\n",
    "        df_b = join_table(df_b, ct, bigram)\n",
    "        df_c = join_table(df_c, ct, bigram)\n",
    "        test = join_table(test, ct, bigram)\n",
    "\n",
    "#    df_a.write.parquet('../data/dfa.parquet', mode='overwrite')\n",
    "#    df_b.write.parquet('../data/dfb.parquet', mode='overwrite')\n",
    "#    df_c.write.parquet('../data/dfc.parquet', mode='overwrite')\n",
    "#    test.write.parquet('../data/test_stack.parquet', mode='overwrite')\n",
    "else:\n",
    "    df_a = spark.read.parquet('../data/dfa.parquet')\n",
    "    df_b = spark.read.parquet('../data/dfb.parquet')\n",
    "    df_c = spark.read.parquet('../data/dfc.parquet')\n",
    "    test = spark.read.parquet('../data/test_stack.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_a = df_a.fillna(0)\n",
    "df_b = df_b.fillna(0)\n",
    "df_c = df_c.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1827229|\n",
      "|            0|5069713|\n",
      "+-------------+-------+\n",
      "\n",
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1828581|\n",
      "|            0|5075130|\n",
      "+-------------+-------+\n",
      "\n",
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1825068|\n",
      "|            0|5070512|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sdf in [ df_a, df_b, df_c ]:\n",
    "    sdf.groupby('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model data in format expected by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['os_pct', 'channel_pct', 'app_pct', 'device_app', 'channel_app', 'ip_pct']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = [ c + '_pct' for c in unigrams ]\n",
    "input_cols += [ '_'.join(b) for b in bigrams ]\n",
    "input_cols += ['ip_pct' ]\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=input_cols, outputCol = 'features')\n",
    "    \n",
    "if True:\n",
    "    model_a = vec_assembler.transform(df_a).select('is_attributed', 'features')\n",
    "    model_b = vec_assembler.transform(df_b).select('is_attributed', 'features')\n",
    "    model_c = vec_assembler.transform(df_c).select('is_attributed', 'features')\n",
    "    \n",
    " #   model_a.write.parquet('../data/model_a.parquet', mode='overwrite')\n",
    " #   model_b.write.parquet('../data/model_b.parquet', mode='overwrite')\n",
    " #   model_c.write.parquet('../data/model_c.parquet', mode='overwrite')\n",
    "else:\n",
    "    model_a = spark.read.parquet('../data/model_a.parquet')\n",
    "    model_b = spark.read.parquet('../data/model_b.parquet')\n",
    "    model_c = spark.read.parquet('../data/model_c.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'is_attributed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtc = GBTClassifier(\n",
    "    labelCol = 'is_attributed',\n",
    ")\n",
    "\n",
    "pg = ParamGridBuilder(\n",
    "       ).addGrid(\n",
    "                gbtc.maxDepth, [ 8 ]\n",
    "       ).addGrid(\n",
    "                gbtc.subsamplingRate, [ .8  ]\n",
    "       ).addGrid(\n",
    "                gbtc.featureSubsetStrategy, [ '5' ] \n",
    "       ).addGrid(\n",
    "                gbtc.maxBins, [ 64 ]\n",
    "       ).addGrid(\n",
    "                gbtc.stepSize, [ .15 ]\n",
    "       ).addGrid(\n",
    "                gbtc.maxIter, [ 12 ]\n",
    "       ).addGrid(\n",
    "                gbtc.minInstancesPerNode, [ 10 ] \n",
    "       ).build(\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(\n",
    "        estimator = gbtc,\n",
    "        estimatorParamMaps = pg,\n",
    "        evaluator = evaluator,\n",
    "        trainRatio = .8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs_a = tvs.fit(model_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709555119579643"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_a = tvs_a.transform(model_a)\n",
    "evaluator.evaluate(results_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(6, {0: 0.0369, 1: 0.0396, 2: 0.5941, 3: 0.1941, 4: 0.0503, 5: 0.085})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_a.bestModel.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['os_pct', 'channel_pct', 'app_pct', 'device_app', 'channel_app', 'ip_pct']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9708689128214322"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_b = tvs.fit(model_b)\n",
    "results_b = tvs_b.transform(model_b)\n",
    "evaluator.evaluate(results_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.970945178729162"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_c = tvs.fit(model_c)\n",
    "results_c = tvs_c.transform(model_c)\n",
    "evaluator.evaluate(results_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs_a.bestModel.save('../data/tvs_a.model')\n",
    "tvs_b.bestModel.save('../data/tvs_b.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs_c.bestModel.save('../data/tvs_c.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's bring the test set in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = vec_assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = tvs_a.transform(test_model)\n",
    "test_b = tvs_b.transform(test_model)\n",
    "test_c = tvs_c.transform(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sdf):\n",
    "    sdf = sdf.select('click_id', \n",
    "                       F.col('prediction').astype(T.ShortType()), \n",
    "                       'probability')\n",
    "    sdf.groupby('prediction').count().show()\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('channel', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('doy', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('click_id', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('ip_pct', 'double'),\n",
       " ('hour', 'double'),\n",
       " ('os_pct', 'double'),\n",
       " ('channel_pct', 'double'),\n",
       " ('app_pct', 'double'),\n",
       " ('device_app', 'double'),\n",
       " ('channel_app', 'double'),\n",
       " ('features', 'vector'),\n",
       " ('rawPrediction', 'vector'),\n",
       " ('probability', 'vector'),\n",
       " ('prediction', 'double')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_a.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = T.StructType([\n",
    "    T.StructField('click_id', T.IntegerType()),\n",
    "    T.StructField('prediction', T.FloatType()),\n",
    "    T.StructField('pclass1', T.FloatType())\n",
    "])\n",
    "\n",
    "def save_stuff(x):\n",
    "    return T.Row(click_id=x.click_id, \n",
    "                prediction=x.prediction, \n",
    "                pclass1=float(x.probability[1]))\n",
    "\n",
    "vec_a = test_a.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)\n",
    "vec_b = test_a.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)\n",
    "vec_c = test_a.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the median of the three models as my final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_a = vec_a.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_a') )\n",
    "vec_b = vec_b.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_b') )\n",
    "vec_c = vec_c.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_c') )\n",
    "\n",
    "joined = vec_a.join(vec_b, ['click_id']).join(vec_c, ['click_id'])\n",
    "\n",
    "mySchema = T.StructType([\n",
    "    T.StructField('click_id', T.IntegerType()),\n",
    "    T.StructField('is_attributed', T.FloatType())\n",
    "])\n",
    "\n",
    "from statistics import median\n",
    "def get_predict(x):\n",
    "    return T.Row(click_id=x.click_id,\n",
    "                is_attributed=median([x.vec_a, x.vec_b, x.vec_c]))\n",
    "\n",
    "joined = joined.rdd.map(lambda x: get_predict(x)).toDF(schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.write.csv('../data/one_last_tiimmee.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
