{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What am I doing here?\n",
    "\n",
    "- Loading the prefitted models from the training subsets, and running the\n",
    "results on all 185 million rows of the training set.\n",
    "\n",
    "TODO: A lot of cut-n-paste from the model-building notebook - should refactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.classification import GBTClassifier, GBTClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark 2.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is optional stuff - either pip install watermark\n",
    "# or just comment it out (it just keeps track of what library\n",
    "# versions I have)\n",
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '8g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [ 'os', 'channel', 'app' ]\n",
    "bigrams = [['device', 'app'], \n",
    "           ['channel', 'app']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/train.csv',  header=True, inferSchema=True)\n",
    "df = df.drop('attributed_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily IP prevalence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('doy', F.dayofyear('click_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ip_counts = spark.read.parquet('../data/train_ip.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(\n",
    "    df_ip_counts[['doy','ip','ip_pct']],\n",
    "    on=['doy','ip'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the other count tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_table( group ):\n",
    "    if type(group) == str:\n",
    "        column_name = group + '_pct' # for example: ip_pct\n",
    "    else:\n",
    "        column_name = \"_\".join(group)  # for example: device_os\n",
    "        \n",
    "    table_name = 'table_' + column_name\n",
    "    counts_sdf = spark.read.parquet(f'../data/{table_name}.parquet')\n",
    "    return counts_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_table( sdf, count_table, group ):\n",
    "    sdf = sdf.join(count_table, group, how='left')\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the count columns with the training data \n",
    "# write everything out to disk so we don't have to redo \n",
    "# feature engineering when all I want to do is tune hyperparameters\n",
    "for c in unigrams:\n",
    "    ct   = get_count_table( c )\n",
    "    df   = join_table(df, ct, [c])\n",
    "    \n",
    "for bigram in bigrams:\n",
    "    ct = get_count_table( bigram )\n",
    "    df = join_table(df, ct, bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|is_attributed|    count|\n",
      "+-------------+---------+\n",
      "|            1|   456846|\n",
      "|            0|184447044|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input in format expected by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['os_pct', 'channel_pct', 'app_pct', 'device_app', 'channel_app', 'ip_pct']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols = [ c + '_pct' for c in unigrams ]\n",
    "input_cols += [ '_'.join(b) for b in bigrams ]\n",
    "input_cols += ['ip_pct' ]\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=input_cols, outputCol = 'features')\n",
    "df_in = vec_assembler.transform(df).select('is_attributed', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'is_attributed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT Model written to disk during Bestof.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtc_a = GBTClassificationModel.read().load(\n",
    "    '../data/tvs_a.model')\n",
    "gbtc_b = GBTClassificationModel.read().load(\n",
    "    '../data/tvs_b.model')\n",
    "gbtc_c = GBTClassificationModel.read().load(\n",
    "    '../data/tvs_c.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707573820497113"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_a = gbtc_a.transform(df_in)\n",
    "evaluator.evaluate(results_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707904513351963"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_b = gbtc_b.transform(df_in)\n",
    "evaluator.evaluate(results_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9708539571530767"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_c = gbtc_c.transform(df_in)\n",
    "evaluator.evaluate(results_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9708005968459947"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean([0.9707573820497113, 0.9707904513351963, 0.9708539571530767])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_stats(sdf):\n",
    "    ans = sdf.select('is_attributed', \n",
    "                       F.col('prediction').astype(T.IntegerType()), \n",
    "                    ).groupby(['is_attributed','prediction']\n",
    "                    ).count(\n",
    "                    ).collect(\n",
    "                    )\n",
    "    tp,fp,tn,fn = 0,0,0,0\n",
    "    for row in ans:\n",
    "        if row.is_attributed == 1:\n",
    "            if row.prediction == 0:\n",
    "                fn = row['count']\n",
    "            elif row.prediction == 1:\n",
    "                tp = row['count']\n",
    "        elif row.is_attributed == 0:\n",
    "            if row.prediction == 0:\n",
    "                tn = row['count']\n",
    "            elif row.prediction == 1:\n",
    "                fp = row['count']\n",
    "    assert(tp != 0)\n",
    "    assert(tn != 0)\n",
    "    assert(fp != 0)\n",
    "    assert(fn != 0)\n",
    "    # precision = true_pos / (true_pos + false_pos)\n",
    "    precision = float(tp)/float(tp + fp)\n",
    "    # recall = true_pos / pos\n",
    "    recall = float(tp) /float(tp + fn)\n",
    "    # accuracy \n",
    "    acc = float(tp + tn)/float(sum([fn,tp,tn,fp]))\n",
    "    return precision, recall, acc\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go have a coffee... take a shower... \n",
    "print(\".\")\n",
    "pre_a, rec_a, acc_a = gather_stats(results_a)\n",
    "print(\"..\")\n",
    "pre_b, rec_b, acc_b = gather_stats(results_b)\n",
    "print(\"...\")\n",
    "pre_c, rec_c, acc_c = gather_stats(results_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11577750911463025, 0.11838944905689125, 0.11741209049192722)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_a, pre_b, pre_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8581863472592515, 0.8572100883010905, 0.8566300241219141)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_a, rec_b, rec_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9834560430286242, 0.9838756393929841, 0.9837360641790717)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_a, acc_b, acc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg precision 0.11719301622114957\n",
      "avg recall 0.8573421532274187\n",
      "avg accuracy 0.9836892488668934\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "print('avg precision', mean([pre_a, pre_b, pre_c]))\n",
    "print('avg recall', mean([rec_a, rec_b, rec_c]))\n",
    "print('avg accuracy', mean([acc_a, acc_b, acc_c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop me before I hit return too many times and kill the spark session! \n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
