{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "pyspark 2.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is optional stuff - either pip install watermark\n",
    "# or just comment it out (it just keeps track of what library\n",
    "# versions I have)\n",
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x11060c358>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '10g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1 \n",
    "\n",
    "Read the csv file, drop the attributed_time (because I didn't use it in the MVP),\n",
    "and downsample the 0 class by 50% because I'm still on my laptop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = spark.read.csv('../data/train.csv', \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "    df = df.drop('attributed_time')\n",
    "    df = df.sampleBy('is_attributed', fractions={0:.5,1:1.})\n",
    "\n",
    "    df.write.parquet('../data/checkpoint1.parquet', mode='overwrite')\n",
    "else:\n",
    "    df = spark.read.parquet('../data/checkpoint1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2: Time bins\n",
    "\n",
    "Putting the times into bins with KMeans is also a demo of how to do\n",
    "machine learning in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False:\n",
    "    df = df.withColumn('minute', \n",
    "             (F.hour('click_time') * 60 + F.minute('click_time')).cast(T.FloatType()) )\n",
    "    df = df.withColumn('doy', F.dayofyear('click_time'))\n",
    "\n",
    "    vec_assember = VectorAssembler(inputCols=['minute'], outputCol='features')\n",
    "    df = vec_assember.transform(df)\n",
    "\n",
    "    time_binarizer = KMeans( featuresCol='features', predictionCol='time_bin', k=10)\n",
    "    model = time_binarizer.fit(df.select('features'))\n",
    "    model.save('../data/fitted_time_binarizer.ml')\n",
    "\n",
    "    df = model.transform(df)\n",
    "    df = df.drop('minute')\n",
    "    df = df.drop('features')\n",
    "    df = df.drop('click_time')\n",
    "    \n",
    "    df.write.parquet('../data/checkpoint2.parquet', mode='overwrite')\n",
    "else:\n",
    "    # TODO: Need to read the saved model back in? \n",
    "    df = spark.read.parquet('../data/checkpoint2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+-------+-------------+---+--------+\n",
      "|    ip|app|device| os|channel|is_attributed|doy|time_bin|\n",
      "+------+---+------+---+-------+-------------+---+--------+\n",
      "| 65252| 12|     1| 32|    328|            0|311|       9|\n",
      "| 20173|  3|     1| 19|    280|            0|311|       9|\n",
      "|124715|  8|     1| 19|    145|            0|311|       9|\n",
      "| 81731|  2|     1| 17|    469|            0|311|       9|\n",
      "|147957| 24|     1| 13|    105|            0|311|       9|\n",
      "| 92820| 18|     1| 20|    121|            0|311|       9|\n",
      "|178873|  9|     1| 13|    445|            0|311|       9|\n",
      "|  7059|  2|     1| 36|    435|            0|311|       9|\n",
      "|178023|  2|     1| 25|    477|            0|311|       9|\n",
      "|  1700| 12|     1| 15|    328|            0|311|       9|\n",
      "| 55786| 15|     1| 31|    315|            0|311|       9|\n",
      "| 78223| 18|     1| 18|    107|            0|311|       9|\n",
      "| 77041|  2|     1| 19|    435|            0|311|       9|\n",
      "|203075| 18|     1| 11|    134|            0|311|       9|\n",
      "| 40459| 14|     1| 17|    379|            0|311|       9|\n",
      "| 73516|  3|     1| 22|    280|            0|311|       9|\n",
      "| 90997| 18|     1|  9|    439|            0|311|       9|\n",
      "|104533|  3|     1| 19|    280|            0|311|       9|\n",
      "| 43127|  3|     1| 17|    135|            0|311|       9|\n",
      "| 81859| 12|     1|  6|    265|            0|311|       9|\n",
      "+------+---+------+---+-------+-------------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 3 \n",
    "## Counting and downsampling\n",
    "\n",
    "Really taking stuff away from the MVP model here because it's too hard to \n",
    "make it run on my laptop.  Maybe next time...\n",
    "\n",
    "My first three predictors are:\n",
    "\n",
    " - device/os pair\n",
    " - app\n",
    " - channel\n",
    " \n",
    "Time is also implicitly part of the model becaue I'm binning by \n",
    "calendar day and time before normalizing the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    device_os_count = df.groupby( [ 'device', \n",
    "                                'os',\n",
    "                                'time_bin',\n",
    "                                'doy' ] \n",
    "                     ).count(\n",
    "                     ).withColumnRenamed(\n",
    "                               \"count\", \"dev_os_count\"\n",
    "                     )\n",
    "\n",
    "    app_count = df.groupby(['app', 'time_bin', 'doy']\n",
    "                      ).count(\n",
    "                        ).withColumnRenamed(\n",
    "                            \"count\", \"app_count\"\n",
    "                        )\n",
    "\n",
    "    channel_count = df.groupby(['channel', 'time_bin', 'doy']\n",
    "                      ).count(\n",
    "                        ).withColumnRenamed(\n",
    "                            \"count\", \"channel_count\"\n",
    "                        )\n",
    "\n",
    "    bin_size_count = df.groupby(['time_bin', 'doy']).count()\n",
    "\n",
    "# I should have all the information I need for IDF at this point, so throw away even more data so I can do a join.\n",
    "\n",
    "    df = df.sampleBy('is_attributed', fractions={0:.005,1:1.})\n",
    "    print(df.groupby('is_attributed').count().show())\n",
    "\n",
    "# Merge the munged data and save it as checkpoint 3\n",
    "\n",
    "    device_os_count = device_os_count.join(bin_size_count, on = [ 'doy', 'time_bin' ])\n",
    "    app_count = app_count.join(bin_size_count, on = [ 'doy', 'time_bin' ])\n",
    "    channel_count = channel_count.join(bin_size_count, on = [ 'doy', 'time_bin' ])\n",
    "\n",
    "# The probabilitistic form of IDF (see Wikipedia page) is less sensitive to the size of the corpus,\n",
    "\n",
    "    device_os_count = device_os_count.withColumn('device_os_idf', F.log(F.column('count') - F.column('dev_os_count')) - F.log('dev_os_count'))\n",
    "    app_count = app_count.withColumn('app_idf', F.log(F.column('count') - F.column('app_count')) - F.log('app_count'))\n",
    "    channel_count = channel_count.withColumn('channel_idf', F.log(F.column('count') - F.column('channel_count')) - F.log('channel_count'))\n",
    "\n",
    "    df.createOrReplaceTempView('data')\n",
    "    device_os_count.createOrReplaceTempView('device_os_count')\n",
    "    app_count.createOrReplaceTempView('app_count')\n",
    "    channel_count.createOrReplaceTempView('channel_count')\n",
    "    bin_size_count.createOrReplaceTempView('bin_size_count')\n",
    "\n",
    "    df = spark.sql(\"\"\"\n",
    "SELECT is_attributed, device_os_idf, app_idf, channel_idf\n",
    "FROM data\n",
    "JOIN device_os_count \n",
    "ON   data.doy = device_os_count.doy\n",
    "AND  data.time_bin = device_os_count.time_bin\n",
    "AND  data.device = device_os_count.device\n",
    "JOIN app_count\n",
    "ON   data.doy = app_count.doy\n",
    "AND  data.time_bin = app_count.time_bin\n",
    "AND  data.app = app_count.app\n",
    "JOIN channel_count\n",
    "ON   data.doy = channel_count.doy\n",
    "AND  data.time_bin = channel_count.time_bin\n",
    "AND  data.channel = channel_count.channel\n",
    "\"\"\")\n",
    "\n",
    "    df.write.parquet('../data/checkpoint3.parquet', mode='overwrite')\n",
    "    \n",
    "else:\n",
    "    df = spark.read.parquet('../data/checkpoint3.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+------------------+\n",
      "|is_attributed|     device_os_idf|           app_idf|       channel_idf|\n",
      "+-------------+------------------+------------------+------------------+\n",
      "|            1|3.8025177087881463|1.7919873757069489|10.097685973738646|\n",
      "|            1| 9.669209323473428|1.7919873757069489|10.097685973738646|\n",
      "|            1| 7.200170037028189|1.7919873757069489|10.097685973738646|\n",
      "|            1| 9.337050591247532|1.7919873757069489|10.097685973738646|\n",
      "|            1|10.563064544138223|1.7919873757069489|10.097685973738646|\n",
      "|            1|4.7103701220476815|1.7919873757069489|10.097685973738646|\n",
      "|            1| 11.77947806150139|1.7919873757069489|10.097685973738646|\n",
      "|            1| 11.77947806150139|1.7919873757069489|10.097685973738646|\n",
      "|            1|11.556332595146063|1.7919873757069489|10.097685973738646|\n",
      "|            1| 4.890197679790861|1.7919873757069489|10.097685973738646|\n",
      "|            1|13.165778167722625|1.7919873757069489|10.097685973738646|\n",
      "|            1| 6.710150252834481|1.7919873757069489|10.097685973738646|\n",
      "|            1| 7.662012886422265|1.7919873757069489|10.097685973738646|\n",
      "|            1| 5.885757469062338|1.7919873757069489|10.097685973738646|\n",
      "|            1|12.472629072132564|1.7919873757069489|10.097685973738646|\n",
      "|            1| 4.812347503651878|1.7919873757069489|10.097685973738646|\n",
      "|            1|13.165778167722625|1.7919873757069489|10.097685973738646|\n",
      "|            1| 10.49160366508012|1.7919873757069489|10.097685973738646|\n",
      "|            1| 13.85892630579625|1.7919873757069489|10.097685973738646|\n",
      "|            1| 13.85892630579625|1.7919873757069489|10.097685973738646|\n",
      "+-------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('data')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Random Forest model\n",
    "\n",
    "I'm still not doing full cross-validation here, but one split is better than nothing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    vec_assember = VectorAssembler(inputCols=['device_os_idf', 'app_idf', 'channel_idf'], \n",
    "                               outputCol='features')\n",
    "    df = vec_assember.transform(df)\n",
    "\n",
    "# Default metric in pyspark is ROC AUC\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol = 'is_attributed')\n",
    "\n",
    "    rfc = RandomForestClassifier(\n",
    "        featuresCol = 'features',\n",
    "        labelCol = 'is_attributed',\n",
    "        numTrees = 20\n",
    "    )\n",
    "\n",
    "    pg = ParamGridBuilder(\n",
    "       ).addGrid(\n",
    "                rfc.minInstancesPerNode, [100]\n",
    "       ).addGrid(\n",
    "                rfc.maxDepth, [4,5]\n",
    "       ).addGrid(\n",
    "                rfc.featureSubsetStrategy, ['2','3']\n",
    "       ).addGrid(\n",
    "                rfc.subsamplingRate, [.75, .87,  1.]\n",
    "       ).build(\n",
    "       )\n",
    "\n",
    "    tvs = TrainValidationSplit(\n",
    "            estimator = rfc,\n",
    "            estimatorParamMaps = pg,\n",
    "            evaluator = evaluator,\n",
    "            trainRatio = .8\n",
    "    )\n",
    "\n",
    "    tvs_model = tvs.fit(df)\n",
    "    df = tvs_model.transform(df)\n",
    "    evaluator.evaluate(df)\n",
    "\n",
    "    print(tvs_model.bestModel.explainParams())\n",
    "# minIntances = 100, maxDepth =4 (increased to 5 for next time), featureSubsetStrategy = 2 (add 3 as a choice next time),\n",
    "# subsamplingRate wasn't set, add some choices for next time.\n",
    "\n",
    "    tvs_model.bestModel.save('../data/TrainedSparkRF')\n",
    "else:\n",
    "    pass\n",
    "    # TODO : Figure out how to read the model back in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up -- \n",
    "\n",
    "- Load the test set \n",
    "- Feature engineering\n",
    "- See if training and test data seem to be of similar distribution\n",
    "- If so, run a prediction and upload it to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
