{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What am I doing here?\n",
    "\n",
    "1. Create all the bigram counts\n",
    "2. Cut them off at the top 100 each\n",
    "3. Merge them back in \n",
    "4. Run through a random forest\n",
    "5. See if it cross-validates\n",
    "6. See if you can upload a guess to Kaggle and see if it generalizes at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "pyspark 2.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is optional stuff - either pip install watermark\n",
    "# or just comment it out (it just keeps track of what library\n",
    "# versions I have)\n",
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x11f19b320>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '10g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1 \n",
    "\n",
    "Read the csv file, drop the attributed_time (because I didn't use it in the MVP),\n",
    "and downsample the 0 class to 25% because I'm still on my laptop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = spark.read.csv('../data/train.csv', \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "    df = df.drop('attributed_time')\n",
    "    df = df.sampleBy('is_attributed', fractions={0:.25,1:1.})\n",
    "    \n",
    "    test = spark.read.csv('../data/test.csv', \n",
    "                         header= True, inferSchema=True)\n",
    "\n",
    "    df.write.parquet('../data/checkpoint1.parquet', mode='overwrite')\n",
    "    test.write.parquet('../data/test_checkpoint1.parquet', mode='overwrite')\n",
    "else:\n",
    "    df = spark.read.parquet('../data/checkpoint1.parquet')\n",
    "    test = spark.read.parquet('../data/test_checkpoint1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('click_id', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46575441"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2\n",
    "## Counting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_column( sdf, groupby_clause ):\n",
    "    \n",
    "    if type(groupby_clause) == str:\n",
    "        column_name = groupby_clause + '_pct' # for example: ip_pct\n",
    "        join_clause = [groupby_clause]\n",
    "    else:\n",
    "        column_name = \"_\".join(groupby_clause)  # for example: device_os\n",
    "        join_clause = groupby_clause\n",
    "\n",
    "    counts_sdf =  sdf.groupby( \n",
    "                        groupby_clause \n",
    "                ).count(\n",
    "                ).orderBy(\n",
    "                    'count', ascending = False\n",
    "                ).limit(\n",
    "                    100 # so we don't chase the \"long tail\"\n",
    "                )\n",
    "    \n",
    "    maxcnt = counts_sdf.select(F.max('count').alias('maxcnt')).collect()\n",
    "    maxcnt = maxcnt[0].maxcnt\n",
    "    \n",
    "    counts_sdf = counts_sdf.withColumn('ratios',\n",
    "                    F.col('count').astype(T.DoubleType())/float(maxcnt))\n",
    "    counts_sdf = counts_sdf.drop('count').withColumnRenamed('ratios', column_name)\n",
    "    \n",
    "    table_name = 'table_' + column_name\n",
    "    counts_sdf.createOrReplaceTempView(table_name)\n",
    "    \n",
    "    sdf = sdf.join(\n",
    "            counts_sdf,\n",
    "            groupby_clause,\n",
    "            how='left'\n",
    "    )\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create percents from training data \n",
    "\n",
    "columns = [ 'device', 'os', 'ip', 'channel', 'app' ]\n",
    "bigrams = [ list(b) for b in combinations(columns,2)]\n",
    "\n",
    "if False:\n",
    "    \n",
    "# create the count columns with the training data \n",
    "\n",
    "    for c in columns:\n",
    "        df = make_count_column( df, c )\n",
    "    \n",
    "    for bigram in combinations(columns, 2):\n",
    "        df = make_count_column( df, list(bigram) )\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "# merge them with test data\n",
    "\n",
    "    unigram_columns = columns\n",
    "    unigram_tables = [ 'table_' + c + '_pct' for c in columns ]\n",
    "\n",
    "    for table_name, column_name in zip(unigram_tables, unigram_columns):\n",
    "        pct_column_name = column_name + '_pct'\n",
    "        udf = spark.table(table_name)\n",
    "        test = test.join(udf,\n",
    "                     [column_name],\n",
    "                     how = 'left'\n",
    "                    )\n",
    "\n",
    "    bigrams = [ list(b) for b in combinations(columns,2)]\n",
    "    big_tables = [ 'table_' + '_'.join(b) for b in bigrams ]\n",
    "\n",
    "    for table_name, bigram in zip(big_tables, bigrams):\n",
    "        column_name = '_'.join(bigram)\n",
    "        udf = spark.table(table_name)\n",
    "        test = test.join(udf,\n",
    "                    bigram,\n",
    "                    how = 'left')\n",
    "    \n",
    "    test = test.fillna(0)\n",
    "\n",
    "# checkpoint the train and test counts\n",
    "\n",
    "    df.write.parquet('../data/df_counts.parquet', mode='overwrite')\n",
    "    test.write.parquet('../data/test_counts.parquet', mode='overwrite')\n",
    "else:\n",
    "    df = spark.read.parquet('../data/df_counts.parquet')\n",
    "    test = spark.read.parquet('../data/test_counts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('channel', 'int'),\n",
       " ('app', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('os', 'int'),\n",
       " ('device', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int'),\n",
       " ('device_pct', 'double'),\n",
       " ('os_pct', 'double'),\n",
       " ('ip_pct', 'double'),\n",
       " ('channel_pct', 'double'),\n",
       " ('app_pct', 'double'),\n",
       " ('device_os', 'double'),\n",
       " ('device_ip', 'double'),\n",
       " ('device_channel', 'double'),\n",
       " ('device_app', 'double'),\n",
       " ('os_ip', 'double'),\n",
       " ('os_channel', 'double'),\n",
       " ('os_app', 'double'),\n",
       " ('ip_channel', 'double'),\n",
       " ('ip_app', 'double'),\n",
       " ('channel_app', 'double')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('channel', 'int'),\n",
       " ('app', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('os', 'int'),\n",
       " ('device', 'int'),\n",
       " ('click_id', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('device_pct', 'double'),\n",
       " ('os_pct', 'double'),\n",
       " ('ip_pct', 'double'),\n",
       " ('channel_pct', 'double'),\n",
       " ('app_pct', 'double'),\n",
       " ('device_os', 'double'),\n",
       " ('device_ip', 'double'),\n",
       " ('device_channel', 'double'),\n",
       " ('device_app', 'double'),\n",
       " ('os_ip', 'double'),\n",
       " ('os_channel', 'double'),\n",
       " ('os_app', 'double'),\n",
       " ('ip_channel', 'double'),\n",
       " ('ip_app', 'double'),\n",
       " ('channel_app', 'double')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A little class balancing on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|is_attributed|   count|\n",
      "+-------------+--------+\n",
      "|            1|  456846|\n",
      "|            0|46118595|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            0|5072437|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class0 = df.sampleBy('is_attributed', {0:.11})\n",
    "class0.groupby('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = df.filter(df.is_attributed == 1).sample(withReplacement=True, fraction=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1825575"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5072437"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = class1.unionAll(class0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now we model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping IP because other teams have found it doesn't generalize well to\n",
    "# the test set. (Need to go back and delete from checkpoints to save \n",
    "# time)\n",
    "\n",
    "columns = [ 'device', 'os', 'channel', 'app' ]\n",
    "bigrams = [ list(b) for b in combinations(columns,2)]\n",
    "\n",
    "input_cols = [ c + '_pct' for c in columns ]\n",
    "input_cols += [ '_'.join(b) for b in bigrams ]\n",
    "vec_assembler = VectorAssembler(inputCols=input_cols, outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = vec_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('channel', 'int'),\n",
       " ('app', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('os', 'int'),\n",
       " ('device', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int'),\n",
       " ('device_pct', 'double'),\n",
       " ('os_pct', 'double'),\n",
       " ('ip_pct', 'double'),\n",
       " ('channel_pct', 'double'),\n",
       " ('app_pct', 'double'),\n",
       " ('device_os', 'double'),\n",
       " ('device_ip', 'double'),\n",
       " ('device_channel', 'double'),\n",
       " ('device_app', 'double'),\n",
       " ('os_ip', 'double'),\n",
       " ('os_channel', 'double'),\n",
       " ('os_app', 'double'),\n",
       " ('ip_channel', 'double'),\n",
       " ('ip_app', 'double'),\n",
       " ('channel_app', 'double'),\n",
       " ('features', 'vector')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol = 'is_attributed')\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    labelCol = 'is_attributed',\n",
    ")\n",
    "\n",
    "# Preparting for future hyperparameter tuning\n",
    "pg = ParamGridBuilder(\n",
    "       ).addGrid(\n",
    "                rfc.numTrees, [25]\n",
    "       ).addGrid(\n",
    "                rfc.maxDepth, [5,7]\n",
    "       ).addGrid(\n",
    "                rfc.subsamplingRate, [.55 ]\n",
    "       ).build(\n",
    "       )\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "        estimator = rfc,\n",
    "        estimatorParamMaps = pg,\n",
    "        evaluator = evaluator,\n",
    "        trainRatio = .8\n",
    "    )\n",
    "\n",
    "tvs_model = tvs.fit(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tvs_model.transform(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475162656537023"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's bring the test set in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = vec_assembler.transform(test)\n",
    "results = tvs_model.transform(test_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('channel', 'int'),\n",
       " ('app', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('os', 'int'),\n",
       " ('device', 'int'),\n",
       " ('click_id', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('device_pct', 'double'),\n",
       " ('os_pct', 'double'),\n",
       " ('ip_pct', 'double'),\n",
       " ('channel_pct', 'double'),\n",
       " ('app_pct', 'double'),\n",
       " ('device_os', 'double'),\n",
       " ('device_ip', 'double'),\n",
       " ('device_channel', 'double'),\n",
       " ('device_app', 'double'),\n",
       " ('os_ip', 'double'),\n",
       " ('os_channel', 'double'),\n",
       " ('os_app', 'double'),\n",
       " ('ip_channel', 'double'),\n",
       " ('ip_app', 'double'),\n",
       " ('channel_app', 'double'),\n",
       " ('features', 'vector'),\n",
       " ('rawPrediction', 'vector'),\n",
       " ('probability', 'vector'),\n",
       " ('prediction', 'double')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.select('click_id', F.col('prediction').astype(T.ShortType()), 'probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('click_id', 'int'), ('prediction', 'smallint'), ('probability', 'vector')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|  623912|\n",
      "|         0|18166557|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.groupby('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.write.parquet('../data/results.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = T.StructType([\n",
    "    T.StructField('click_id', T.IntegerType()),\n",
    "    T.StructField('prediction', T.ShortType()),\n",
    "    T.StructField('pclass1', T.FloatType())\n",
    "])\n",
    "\n",
    "def save_stuff(x):\n",
    "    return T.Row(click_id=x.click_id, \n",
    "                prediction=x.prediction, \n",
    "                pclass1=float(x.probability[1]))\n",
    "\n",
    "extract_vector = results.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|click_id|prediction|    pclass1|\n",
      "+--------+----------+-----------+\n",
      "|14219477|         0| 0.04365348|\n",
      "|14219478|         0|0.038334113|\n",
      "|14219479|         0| 0.06678616|\n",
      "|14219480|         0|0.070457734|\n",
      "|14219481|         0| 0.07859021|\n",
      "+--------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_vector.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't write the CSV with a header, so the alias is_attributed isn't really needed\n",
    "extract_vector.select('click_id', \n",
    "                      F.col('pclass1').alias('is_attributed')\n",
    "             ).write.csv('../data/submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
