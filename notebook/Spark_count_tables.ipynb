{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What am I doing here?\n",
    "\n",
    "- Counting the prevalence of various features (the ones that were predictive in earlier mvp models)\n",
    "- Each count is normalized by the max count found\n",
    "- All the count tables are written out as parquet files so I don't have to count them for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark 2.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is optional stuff - either pip install watermark\n",
    "# or just comment it out (it just keeps track of what library\n",
    "# versions I have)\n",
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x1080edba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '8g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables for my features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [ 'os', 'channel', 'app' ]\n",
    "bigrams = [ ['device', 'app'], \n",
    "           ['channel', 'app'],\n",
    "           ['device', 'hour'],\n",
    "           ['app', 'hour']\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in all the stuff I want to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.csv('../data/train.csv', \n",
    "                    header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop('attributed_time')\n",
    "train = train.drop('is_attributed')\n",
    "all_data = train\n",
    "all_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour(sdf):\n",
    "    return sdf.withColumn('hour',\n",
    "                F.hour('click_time').astype(T.ShortType()))\n",
    "    \n",
    "all_data = add_hour(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make count tables\n",
    "\n",
    "Just using the columns from the best-so-far model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_table( groupby_clause ):\n",
    "    \n",
    "    if type(groupby_clause) == str:\n",
    "        column_name = groupby_clause + '_pct' # for example: ip_pct\n",
    "    else:\n",
    "        column_name = \"_\".join(groupby_clause)  # for example: device_os\n",
    "        \n",
    "    counts_sdf =  all_data.groupby( \n",
    "                        groupby_clause \n",
    "                ).count(\n",
    "                ).orderBy(\n",
    "                    'count', ascending = False\n",
    "                )\n",
    "    \n",
    "    maxcnt = counts_sdf.select(F.max('count').alias('maxcnt')).collect()\n",
    "    maxcnt = maxcnt[0].maxcnt\n",
    "    \n",
    "    counts_sdf = counts_sdf.withColumn('ratios',\n",
    "                    F.col('count').astype(T.DoubleType())/float(maxcnt))\n",
    "    counts_sdf = counts_sdf.drop('count').withColumnRenamed('ratios', column_name)\n",
    "    \n",
    "    table_name = 'table_' + column_name\n",
    "    counts_sdf.createOrReplaceTempView(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create all the parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in unigrams:\n",
    "    make_count_table( c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram in bigrams:\n",
    "    make_count_table( bigram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_count_table(['device', 'hour'])\n",
    "make_count_table(['app', 'hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tables = [ 'table_' + c + '_pct' for c in unigrams ]\n",
    "big_tables = [ 'table_' + '_'.join(b) for b in bigrams ]\n",
    "all_tables = unigram_tables + big_tables\n",
    "\n",
    "for table in all_tables:\n",
    "    df = spark.table(table)\n",
    "    df.write.parquet(f'../data/{table}.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table('table_device_hour')\n",
    "df.write.parquet('../data/table_device_hour.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table('table_app_hour')\n",
    "df.write.parquet('../data/table_app_hour.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.csv('../data/test.csv', header=True,\n",
    "                     inferSchema=True)\n",
    "\n",
    "# add date column using day of year function\n",
    "train = train.withColumn('doy', F.dayofyear('click_time'))\n",
    "test = test.withColumn('doy', F.dayofyear('click_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('doy', 'int')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ip_counts(sdf):\n",
    "    ipday = ['doy', 'ip']\n",
    "    # count how many times an ip appears each day\n",
    "    day_counts = sdf[ipday].groupby(ipday).count()\n",
    "    # find the max count for each day\n",
    "    day_max = day_counts[['doy','count']]\\\n",
    "                    .groupby(['doy'])\\\n",
    "                    .max()\\\n",
    "                    .withColumnRenamed('max(count)', 'day_max')\\\n",
    "                    .drop('max(doy)')\n",
    "    # merge the max per day into the daily counts table\n",
    "    merge = day_counts.join(day_max, ['doy'], how='left')\n",
    "    # normalize all the counts by the max\n",
    "    ip_table = merge.withColumn('ip_pct',\n",
    "                     F.col('count').astype(T.FloatType())/\n",
    "                     F.col('day_max').astype(T.FloatType())\n",
    "                    ).drop(\n",
    "                        'count'\n",
    "                    ).drop(\n",
    "                        'day_max'\n",
    "                    )\n",
    "    return ip_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ip = make_ip_counts(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ip = make_ip_counts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ip.write.parquet('../data/train_ip.parquet',\n",
    "                      mode='overwrite')\n",
    "test_ip.write.parquet('../data/test_ip.parquet',\n",
    "                        mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
