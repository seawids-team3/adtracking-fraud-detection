{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What am I doing here?\n",
    "\n",
    "- GBT model\n",
    "- Setting up doing three runs, can change to three different models for a voting classifier\n",
    "- Every time I checkpoint a step to a file, it's in an 'if False' block. If you need to\n",
    "create a file, change that to True to make the file. Then change it back to False to \n",
    "get the faster way through the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "pyspark 2.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is optional stuff - either pip install watermark\n",
    "# or just comment it out (it just keeps track of what library\n",
    "# versions I have)\n",
    "%load_ext watermark\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x116dc5470>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comment these out to run on a cluster. Also, adjust memory to size of your laptop\n",
    "pyspark.sql.SparkSession.builder.config('spark.driver.memory', '8g')\n",
    "pyspark.sql.SparkSession.builder.config('spark.sql.shuffle.paritions', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [ 'os', 'channel', 'app' ]\n",
    "bigrams = [[ 'device', 'os'], \n",
    "           ['device', 'channel'], \n",
    "           ['device', 'app'], \n",
    "           ['channel', 'app']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1 \n",
    "\n",
    "Read the csv file, drop the attributed_time (because I didn't use it in the MVP),\n",
    "and downsample the 0 class to 25% because I'm still on my laptop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = spark.read.csv('../data/train.csv', \n",
    "                    header=True, inferSchema=True)\n",
    "\n",
    "    df = df.drop('attributed_time')\n",
    "    df = df.sampleBy('is_attributed', fractions={0:.25,1:1.})\n",
    "    \n",
    "    test = spark.read.csv('../data/test.csv', \n",
    "                         header= True, inferSchema=True)\n",
    "\n",
    "    df.write.parquet('../data/checkpoint1.parquet', mode='overwrite')\n",
    "    test.write.parquet('../data/test_checkpoint1.parquet', mode='overwrite')\n",
    "else:\n",
    "    df = spark.read.parquet('../data/checkpoint1.parquet')\n",
    "    test = spark.read.parquet('../data/test_checkpoint1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp'),\n",
       " ('is_attributed', 'int')]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('click_id', 'int'),\n",
       " ('ip', 'int'),\n",
       " ('app', 'int'),\n",
       " ('device', 'int'),\n",
       " ('os', 'int'),\n",
       " ('channel', 'int'),\n",
       " ('click_time', 'timestamp')]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46575441"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily IP prevalence \n",
    "Because IP addresses get reassigned, need to do these as feature engineering on train and test\n",
    "sets separately.\n",
    "(See the link Elyse posted on the slack.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('doy', \n",
    "                   F.dayofyear('click_time'))\n",
    "test = test.withColumn('doy',\n",
    "                   F.dayofyear('click_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ip_counts  = df[['doy', 'ip']].groupby(['doy','ip']).count()\n",
    "test_ip_counts = test[['doy', 'ip']].groupby(['doy', 'ip']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_day_max = df_ip_counts[['doy','count']]\\\n",
    "                .groupby(['doy'])\\\n",
    "                .max()\\\n",
    "                .withColumnRenamed('max(count)', 'day_max')\\\n",
    "                .drop('max(doy)')\n",
    "test_day_max = test_ip_counts[['doy','count']]\\\n",
    "                .groupby(['doy'])\\\n",
    "                .max()\\\n",
    "                .withColumnRenamed('max(count)', 'day_max')\\\n",
    "                .drop('max(doy)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ip_counts = df_ip_counts.join(df_day_max,\n",
    "                                ['doy'],\n",
    "                                how='left')\n",
    "test_ip_counts = test_ip_counts.join(test_day_max,\n",
    "                                ['doy'],\n",
    "                                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doy', 'int'), ('ip', 'int'), ('count', 'bigint'), ('day_max', 'bigint')]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ip_counts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ip_counts = df_ip_counts.withColumn('ip_pct',\n",
    "                F.col('count').astype(T.FloatType())/\n",
    "                F.col('day_max').astype(T.FloatType()))\n",
    "\n",
    "test_ip_counts = test_ip_counts.withColumn('ip_pct',\n",
    "                F.col('count').astype(T.FloatType())/\n",
    "                F.col('day_max').astype(T.FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(\n",
    "    df_ip_counts[['doy','ip','ip_pct']],\n",
    "    on=['doy','ip'],\n",
    "    how='left'\n",
    ")\n",
    "test = test.join(\n",
    "    test_ip_counts[['doy','ip','ip_pct']],\n",
    "    on=['doy','ip'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same class balancing as MVP\n",
    "Still hacky - but I reordered it so that the join happens on a\n",
    "smaller table.\n",
    "And, now there are three versions to stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_a = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=111)\n",
    "class1_b = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=222)\n",
    "class1_c = df.filter(df.is_attributed == 1).sample(\n",
    "    withReplacement=True, fraction=4.0, seed=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df.sampleBy('is_attributed', {0:.11}, seed=111).unionAll(class1_a)\n",
    "df_b = df.sampleBy('is_attributed', {0:.11}, seed=222).unionAll(class1_b)\n",
    "df_c = df.sampleBy('is_attributed', {0:.11}, seed=333).unionAll(class1_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting \n",
    "\n",
    "Built count tables except for IP with the full training set rather than the \n",
    "subset. Results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_table( group ):\n",
    "    if type(group) == str:\n",
    "        column_name = group + '_pct' # for example: ip_pct\n",
    "    else:\n",
    "        column_name = \"_\".join(group)  # for example: device_os\n",
    "        \n",
    "    table_name = 'table_' + column_name\n",
    "    counts_sdf = spark.read.parquet(f'../data/{table_name}.parquet')\n",
    "    return counts_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_table( sdf, count_table, group ):\n",
    "    sdf = sdf.join(count_table, group, how='left')\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the count columns with the training data \n",
    "# write everything out to disk so we don't have to redo \n",
    "# feature engineering when all I want to do is tune hyperparameters\n",
    "if False:\n",
    "    for c in unigrams:\n",
    "        ct   = get_count_table( c )\n",
    "        df_a   = join_table(df_a, ct, [c])\n",
    "        df_b   = join_table(df_b, ct, [c])\n",
    "        df_c   = join_table(df_c, ct, [c])\n",
    "        test = join_table(test, ct, [c])\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        ct = get_count_table( bigram )\n",
    "        df_a = join_table(df_a, ct, bigram)\n",
    "        df_b = join_table(df_b, ct, bigram)\n",
    "        df_c = join_table(df_c, ct, bigram)\n",
    "        test = join_table(test, ct, bigram)\n",
    "        \n",
    "    df_a.write.parquet('../data/dfa.parquet', mode='overwrite')\n",
    "    df_b.write.parquet('../data/dfb.parquet', mode='overwrite')\n",
    "    df_c.write.parquet('../data/dfc.parquet', mode='overwrite')\n",
    "    test.write.parquet('../data/test_stack.parquet', mode='overwrite')\n",
    "else:\n",
    "    df_a = spark.read.parquet('../data/dfa.parquet')\n",
    "    df_b = spark.read.parquet('../data/dfb.parquet')\n",
    "    df_c = spark.read.parquet('../data/dfc.parquet')\n",
    "    test = spark.read.parquet('../data/test_stack.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df_a.fillna(0) \n",
    "df_b = df_b.fillna(0) \n",
    "df_c = df_c.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1829362|\n",
      "|            0|5072930|\n",
      "+-------------+-------+\n",
      "\n",
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1825928|\n",
      "|            0|5072188|\n",
      "+-------------+-------+\n",
      "\n",
      "+-------------+-------+\n",
      "|is_attributed|  count|\n",
      "+-------------+-------+\n",
      "|            1|1827785|\n",
      "|            0|5074657|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sdf in [ df_a, df_b, df_c ]:\n",
    "    sdf.groupby('is_attributed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18790469"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last minute model tweak - add hour column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour(sdf):\n",
    "    return sdf.withColumn('hour',\n",
    "                  (F.hour('click_time').astype(T.FloatType()) + \n",
    "                   (F.minute('click_time').astype(T.FloatType()) / 60.)) / 24. )\n",
    "\n",
    "test = add_hour(test)\n",
    "df_a = add_hour(df_a)\n",
    "df_b = add_hour(df_b)\n",
    "df_c = add_hour(df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model data in format expected by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [ c + '_pct' for c in unigrams ]\n",
    "input_cols += [ '_'.join(b) for b in bigrams ]\n",
    "input_cols += ['ip_pct', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['os_pct',\n",
       " 'channel_pct',\n",
       " 'app_pct',\n",
       " 'device_os',\n",
       " 'device_channel',\n",
       " 'device_app',\n",
       " 'channel_app',\n",
       " 'ip_pct',\n",
       " 'hour']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols=input_cols, outputCol = 'features')\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'is_attributed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = vec_assembler.transform(df_a).select('is_attributed', 'features')\n",
    "model_b = vec_assembler.transform(df_b).select('is_attributed', 'features')\n",
    "model_c = vec_assembler.transform(df_c).select('is_attributed', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtc = GBTClassifier(\n",
    "    labelCol = 'is_attributed',\n",
    ")\n",
    "\n",
    "# Preparting for future hyperparameter tuning\n",
    "pg = ParamGridBuilder(\n",
    "       ).addGrid(\n",
    "                gbtc.maxDepth, [ 10 ]\n",
    "       ).addGrid(\n",
    "                gbtc.subsamplingRate, [ .8  ]\n",
    "       ).addGrid(\n",
    "                gbtc.featureSubsetStrategy, [ '6' ] \n",
    "       ).addGrid(\n",
    "                gbtc.maxBins, [ 64 ]\n",
    "       ).addGrid(\n",
    "                gbtc.stepSize, [ .2 ]\n",
    "       ).addGrid(\n",
    "                gbtc.maxIter, [ 30 ]\n",
    "       ).build(\n",
    "       )\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "        estimator = gbtc,\n",
    "        estimatorParamMaps = pg,\n",
    "        evaluator = evaluator,\n",
    "        trainRatio = .8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766076828232535"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_a = tvs.fit(model_a)\n",
    "results_a = tvs_a.transform(model_a)\n",
    "evaluator.evaluate(results_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_eb9adceae1b5', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.'): False,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext'): 10,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='featureSubsetStrategy', doc='The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].'): '5',\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='labelCol', doc='label column name'): 'is_attributed',\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: logistic'): 'logistic',\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 64,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='maxIter', doc='maximum number of iterations (>= 0)'): 30,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation.'): 256,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='predictionCol', doc='prediction column name'): 'prediction',\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='seed', doc='random seed'): -4200481452970279265,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.2,\n",
       " Param(parent='GBTClassifier_eb9adceae1b5', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 0.8}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_a.bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9702896606200668"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_b = tvs.fit(model_b)\n",
    "results_b = tvs_b.transform(model_b)\n",
    "evaluator.evaluate(results_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9704599902977329"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvs_c = tvs.fit(model_c)\n",
    "results_c = tvs_c.transform(model_c)\n",
    "evaluator.evaluate(results_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's bring the test set in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = vec_assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_a = tvs_a.transform(test_model)\n",
    "results_b = tvs_b.transform(test_model)\n",
    "results_c = tvs_c.transform(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sdf):\n",
    "    sdf = sdf.select('click_id', \n",
    "                       F.col('prediction').astype(T.ShortType()), \n",
    "                       'probability')\n",
    "    sdf.groupby('prediction').count().show()\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|  561267|\n",
      "|         0|18229202|\n",
      "+----------+--------+\n",
      "\n",
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|  555951|\n",
      "|         0|18234518|\n",
      "+----------+--------+\n",
      "\n",
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|  657822|\n",
      "|         0|18132647|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_a = get_prediction(results_a)\n",
    "results_b = get_prediction(results_b)\n",
    "results_c = get_prediction(results_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = T.StructType([\n",
    "    T.StructField('click_id', T.IntegerType()),\n",
    "    T.StructField('prediction', T.ShortType()),\n",
    "    T.StructField('pclass1', T.FloatType())\n",
    "])\n",
    "\n",
    "def save_stuff(x):\n",
    "    return T.Row(click_id=x.click_id, \n",
    "                prediction=x.prediction, \n",
    "                pclass1=float(x.probability[1]))\n",
    "\n",
    "vec_a = results_a.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)\n",
    "vec_b = results_b.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)\n",
    "vec_c = results_c.rdd.map(lambda x: save_stuff(x)).toDF(schema=mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take the median of the three models as my final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_a = vec_a.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_a') )\n",
    "vec_b = vec_b.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_b') )\n",
    "vec_c = vec_c.select('click_id', \n",
    "                      F.col('pclass1').alias('vec_c') )\n",
    "\n",
    "joined = vec_a.join(vec_b, ['click_id']).join(vec_c, ['click_id'])\n",
    "\n",
    "mySchema = T.StructType([\n",
    "    T.StructField('click_id', T.IntegerType()),\n",
    "    T.StructField('is_attributed', T.FloatType())\n",
    "])\n",
    "\n",
    "from statistics import median\n",
    "def get_predict(x):\n",
    "    return T.Row(click_id=x.click_id,\n",
    "                is_attributed=median([x.vec_a, x.vec_b, x.vec_c]))\n",
    "\n",
    "joined = joined.rdd.map(lambda x: get_predict(x)).toDF(schema=mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.write.csv('../data/vote_results.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
